good morning good
morning welcome to double o six
introduction to algorithms lecture two
i am eric domain and i love algorithms
are you with me yeah
today we're not doing algorithms no
we're doing data structures it's okay
there's lots of algorithms in each data
structure it's like multiple algorithms
for free
we're going to talk about sequences and
sets and
linked lists and dynamic arrays fairly
simple data structures today this
is the beginning of several data
structures we'll be talking about the
next few lectures
uh but before we actually start with one
let me tell you slash remind you
of the idea the difference between an
interface
which you might call an api if you're a
programmer or an adt if you're an
ancient algorithms person like me
versus a data structure
these are useful distinctions uh the
idea is that
an interface says what you want to do a
data structure says how you do it
so you might call this a specification
and in the context of data structures
we're trying to store some data so the
interface will specify
what data you can store
whereas the data structure will give you
an actual representation
and tell you how to store it
this is pretty boring just storing data
is really easy you just
throw it in a file or something what
makes it interesting
is uh having operations on that data
so in the interface you specify
what the operations do what operations
are supported
and some sense what they mean
and the data structure actually gives
you algorithms
this is where the algorithms come in for
how to support those operations
all right so um in this class we're
going to focus on
two main interfaces
and various special cases of them
so the idea is to separate what you want
to do versus how to do it because
for you can think of this as the problem
statement
so yesterday or last class jason talked
about
problems and defined what a problem was
versus algorithmic solutions to the
problem
and this is the analogous notion for
data structures where we want to
maintain a data
maintain some data according to various
operations
so the same problem can be solved by
many different data structures and we're
going to see that
and different data structures are going
to have different advantages they might
support some operations faster than
others
and depending on what you actually use
those data structures for you choose
the right data structure but you can
maintain the same interface
we're going to think about two data
structures one is called a set
and one is called a sequence
these are highly loaded terms set means
something to a mathematician it means
someone else something else to a python
programmer
sequence similarly i guess there's not a
python sequence data type
built in um the idea is we want to store
n
things the things will be fairly
arbitrary
think of them as integers or strings and
on the one hand we care about their
values
and maybe we want to maintain them in
sorted order and be able to search for a
given value
which we'll call a key and on the other
hand we care about representing
a particular sequence that we care about
maybe we want to represent the numbers 5
9 7 in that order and store that
in python you could store that in a list
for example
and it will keep track of that order and
this is the first item the second item
the last item
today we're going to be focusing on this
sequence data structure although at the
end i'll mention
the interface for sets but we're going
to be actually solving sequences today
and in the next several lectures we'll
be bouncing back and forth between these
they're closely related
pretty abstract at the moment on the
other hand we're gonna have two
main
let's call them data structure
tools or approaches
one is arrays
and the other is pointers pointer based
or linked
data structures you may have seen these
they're used a lot in programming of
course but we're going to see both of
these today i'll come back to this
sort of highlight in a moment let's jump
into the sequence interface
which i conveniently have part of here
there's a few different levels of
sequences that we might care about i'm
going to start with the static
sequence interface this is where the
item the number of items doesn't change
uh though the actual items might so here
we have n
items i'm going to label them x 0 to x n
minus 1 as in python
so the number of items is n and the
operations i want to support are build
length iteration get and set
so what do these do build is how you get
started to build
a data structure in this interface you
call build of
x exactly how you specify x isn't too
important but the idea is i give you
some items in some order
in python this would be an iterable i'm
going to want to also know its length
and i want to make a new data structure
of size n a new static sequence of size
n
that has those items in that order so
that's how you build one of these
because somehow we have to specify n to
this data structure because n is not
going to
be allowed to change i'm going to give
you a length
method um these are
methods are the object-oriented way of
thinking of operations
that your interface supports
so length will just return this fixed
value and it or sequence this is the
sense in which we want to maintain the
order i want to be able to output x0
through xn minus 1 in the sequence order
in that specified order that they were
built in
or that it was changed to so uh
this is going to iterate through all the
items
so it's going to take at least linear
time to output that but more interesting
is we can
dynamically access anywhere in the
middle of the sequence
we can get the item x i given the value
i
and we can change x i to a given new
item
okay so that's called get at and set at
pretty straightforward
this should remind you very closely of
something a data structure so this is an
interface this is
something i might want to solve but what
is the obvious data structure that
solves
this problem yeah a list in python it's
called the list
i prefer to call it an array but to each
their own
we're gonna use this this could mean
many things um
but the uh solution
to this interface problem the
natural solution
is as what i'll call a static array
jason mentioned these in
uh in lecture one uh it's a little
tricky because there are no static
arrays in python there are only dynamic
arrays which is something we will get to
but uh i want to talk about what is a
static array
really and this relates to
our notion of our model of computation
which jason also talked about
which we call the word ram
remember
so the idea in word ram is that your
memory
is an array
of w bit words this is a bit circular
i'm going to define
an array in terms of the word ram which
is defined in terms of arrays but
i think you know the idea so we have
a big memory which goes off to infinity
maybe
it's divided into words
each word here is w bits long
this is word zero word one word two and
you can
access this array randomly random access
memory so i can give you the number five
and get zero one two three
four five the fifth word in this
round that's how actual memories work
uh you can access any of them equally
quickly
okay so that's memory
and so what we want to do is when we say
an array
we want this to be a consecutive
chunk of memory
let me get a color
so let's say i have an array of size
four and it lives
here
jason can't spell but i can't count so i
think that's four
we've got uh so the array starts here
and it ends over here it's of size four
and it's consecutive which means if i
want to
access um
the array at position
at index i then
this is the same thing as accessing my
memory array
at position wherever the array starts
which i'll call the address of the array
in python this is id of array
plus i okay this is just simple offset
arithmetic
if i want to know the zeroth item of the
array it's right here where it starts
the first item is one after that the
second item is one after that
so as long as i store my array
consecutively in memory
i can access the array in constant time
i can do get
at and set at as quickly as i can
randomly access the memory
and get a value or set a value which
we're assuming
is constant time so
[Music]
array access is constant time
okay this is what allows a static array
to actually solve this problem
in constant time per
get at and set at operation so this may
seem simple but we're really going to
need this model and really rely on this
model
increasingly as we get to more
interesting data structures this is the
first time we're actually needing it
uh let's see length is also constant
time
we're just going to store that number n
along with its address
and build is going to take linear time
iteration will take
linear time
okay pretty straightforward i guess one
thing here when defining build i need to
introduce a little bit more of our model
of computation which is
how do you create an array in the
beginning
i claim i can do it in linear time and
that's just part of the model
so this is called the memory allocation
model
there are a few possible choices here
but the cleanest one is just to assume
that you can
allocate an array
of size n
in theta n time
so it takes linear time to make an array
of size n
you could imagine this being constant it
doesn't really matter much but it does
take work and in particular if you just
allocate some chunk of memory you have
no idea whether it's initialized so
initializing that array to zeros
will cost linear time it won't really
matter constant versus linear but a nice
side effect of this model
is that space if you're just allocating
arrays the amount of space you use is at
most the amount of time you use
or i guess big o of that
so that's a nice feature it's pretty
weird if you imagine
it's unrealistic to imagine you can
allocate an array that's
infinite size and then just use a few
items out of it that won't be
give you a good data structure so we'll
assume it costs
to allocate memory and okay great we
solved the static sequence problem
very simple kind of boring these are
optimal running times
now let's make it interesting make sure
i didn't miss anything
and talk about
oh there is one thing i wanted to talk
about
in the word ram a side effect
of this assumption that array access
should take constant time
and that accessing these positions in my
memory
should take constant time is that
we need to assume w
is at least log n or so
w remember is the machine word size in
real computers this is currently 64.
um or 256 and some
bizarre instructions but uh we don't
usually think of the machine as getting
bigger over time
but you should think of the machine as
getting bigger over time this is a
statement
that says the word side has to grow with
n
it might grow faster than log n but it
has to grow at least as fast as log n
why do i say that because if i have n
things that i'm dealing with and here's
the problem size
maybe it's the array i'm trying to store
whatever if i'm having to
deal with n things in my memory at the
very least i need to be able to address
them
i should be able to say give me the ith
one and represent that number i
in a word otherwise because the machine
is designed to only work with w bit
words in constant time
if i want to be able to access the ith
word in constant time i need a word size
that's at least log n just to address
the n
things in my input so this is a totally
reasonable assumption it may seem weird
because you think of a real machine as
having constant size
but you know real machine has constant
size ram also
my machine has 24 gigs of ram or
whatever
that laptop has eight but uh you don't
think of that as changing over time but
of course if you wanted to process a
larger input you would
buy more ram so eventually
when our ends get really really big
we're going to have to increase w just
so we can address that round
that's the the intuition here but this
is a way to bridge
reality which are fixed machines with
theory in
in algorithms we care about scalability
for very large n we want to know what
that growth function is
and ignore the lead constant factor
that's what asymptotic notion notation
is all about
and for that we need a notion of word
size also changing in this asymptotic
way
all right that's uh that will be more
important
next week when we talk about hashing and
why hashing is a reasonable thing to do
but let's move on to dynamic sequences
which is where things get interesting
so i have the update here so we start
with
static sequences so all of these
operations
are still something we want to support
in a dynamic sequence but we add two
dynamic operations
somewhat controversial operations so
exciting
i want to be able to insert in the
middle of my sequence
and i want to be able to delete from the
middle of my sequence so
here's my sequence which i'm going to
think of
in a picture i'm going to draw it as an
array but it's stored however it's
stored we don't know
this is an interface not an
implementation so we have x0
x1 x2 x3
uh and let's say i insert at position
2 so position 2 is here
so i come in with my new x and i would
like
x to be the new x2 but i don't want to
lose any information if i did set at 2
then i would erase this and replace it
with x but i want to do insert at which
means all of these guys conceptually are
going to shift over by one in terms of
their indices
so then i would get this picture that's
one bigger
and now i've got the new x i've got what
was the old x2
which i don't hesitate to call x2
because that's its old name not its new
name i'm going to draw arrows to say
these guys get copied over
uh these ones are definitely unchanged
our new x2 which i'll call x2 prime is x
and this is x3 prime x4 prime
and so on i i want to be careful here
and of course the new n
prime is n plus one
uh i want to be careful about the
labeling because the key
what makes insert ad interesting is that
later when i call get
at it's with the new indexing so
previously
if i did get at 2 i would get this value
and afterwards if i did get at it 2 i
get the new value if i did get at it 3
down here i would get
i get the value that used to be x2 okay
that's maybe hard to track
but this is a useful conceptually very
useful thing to do
especially when you're inserting or
deleting at the ends
so we're going to define in particular
insert and delete
uh first and last
uh these are
sometimes given if if you have an insert
it has an x
if you do a delete it has no argument
so this means insert at the beginning of
the array which would be like adding it
here
and insert last means adding it on here
so insert last doesn't change the
indices of any of the old items that's a
nice feature of insert last insert first
changes all of them they all get
incremented by one
and we're also interested in the similar
things here we could do
get first
or last or
set first or last
okay which are the obvious special cases
of get ad and set out now these special
cases are particularly interesting
in an algorithms context if you were a
mathematician you would say well
why do i even bother this this is just
shorthand for
a particular call to get or set but what
makes it interesting
from a data structure's perspective is
that we care about algorithms for
supporting these operations and maybe
the algorithm for supporting get first
or set first
or in particular insert first or insert
last
might be more efficient maybe we can
solve this problem better
than we can solve insert at so while
ideally we could solve the entire
dynamics sequence interface
constant time per operation that's not
actually possible you can prove that
but special cases of it where we're just
inserting and deleting from the ends say
we can do that so that's why it's
interesting to introduce
special cases that we care about
cool that's the definition of the
dynamic sequence interface
now we're going to actually solve it
so uh our first data structure for this
is called
linked bliss i've taken
probably you've probably seen linked
lists before at some point
but the main new part here is we're
going to actually analyze them
and see how they how efficiently they
implement all these operations we might
care about
so first review what is a linked list
we store our items
in a bunch of nodes
um
each node has an item in it
and a next field
so you could think of these as class
objects with two
class variables the item and and a next
pointer
and we assemble those into this kind of
structure where we
store in the item fields we're going to
store the actual values
that we want to represent in our
sequence x zero through x n minus 1
in order and then we're going to use the
next pointers to link these all together
in that order so the next pointers are
what actually give us the order
and in addition we're going to keep
track of what's called the head of the
list
so the data structure is going to be
represented by head
if you wanted to you could also store
length
so this could be the data structure
itself
and it's pointing to all of these types
of data structures notice we've just
seen
an array based data structure which is
just a static
array and we've seen a pointer-based
data structure
and we're relying on the fact that
pointers
can be stored in a single word which
means we can de-reference them we can
see what's on the other side of the
pointer
in constant time in our word ram model
in reality each of these nodes is stored
somewhere
in the array of the computer
so maybe each one is two words long so
maybe
uh one node is the first node is here
maybe the second node is here the third
node is here
they're in some arbitrary order we're
using this fact
that we can allocate a an array
of size n in linear time in this case
we're going to have a raise of size 2.
we can just say oh please give me a new
array of size 2.
and that will give make us one of these
nodes
and then we're storing pointers pointers
are just indices
into the giant memory array they're just
what is the address of this little array
okay if you've ever wondered how
pointers are implemented they're just
numbers
that say where in memory is this thing
over here and
in memory they're an arbitrary order
this is really nice because it's
easy to manipulate the order of a linked
list without actually physically moving
nodes around whereas arrays
are problematic maybe it's worth
mentioning
let's start analyzing things so we care
about these dynamic sequence
operations and we could try to apply it
to the static array data structure or we
could try
to or sorry we could try to implement
these operations in a static array
it's possible just not going to be very
good
and we can try to implement it with
linked lists and it's also
not going to be that great let's
go over here
our goal is the next data structure
which is dynamic arrays
but linked lists and static arrays each
have their advantages
so let's first analyze dynamic
sequence operations
um first on a static array
and then on a linked list
so in a static array i think you all
see if i try to insert at the beginning
of the static array that's kind of the
worst case
if i insert first then everybody has to
shift over
if i'm going to maintain this invariant
that the ith item in the array
represents i guess i didn't write it
anywhere here
maybe here
static array we're going to maintain
this invariant that a of i
represents x i
okay if i want to maintain that at all
times when i insert a new thing in the
front because the indices of all the
previous items change
i have to spend time to copy those over
you can do it in linear time
but no better okay so
static array
insert and delete
anywhere
cost
and time
actually uh for two reasons uh reason
number one
is that uh if we're near the front
then we have to do shifting
okay what about insert or delete the
last element of an array
is that any easier because then if i
insert the very last element
none of the indices change i'm just
adding a new element
so i don't have to do shifting
so can i do insert and delete last in
constant time in a static array
no because the size is constant so our
model is that
uh remember our allocation model is that
we can allocate a static array
of size n but it's just a size n i can't
just say
please make it bigger by one i need i
need space to store this extra element
and if you think about where things are
in memory when you
call to this memory allocator which is
part of your operating system you say
please give me
some a chunk of memory it's going to
place them in various places in memory
and some of them might be next to each
other
so if i try to grow this array by one
there might already be something
there and that's not possible without
first shifting so even though in the
array i don't have to do any shifting in
memory i might have to do shifting
and that's outside the model so we're
going to stick to this model of just
you can allocate memory you can also
de-allocate memory just to keep space
usage small but the only way to get more
space is to ask for a new array and that
new array won't be contiguous to your
old one
so question
uh what is a dynamic array will be the
next topic so maybe we'll come back to
that
yeah okay so in a static array you're
just not allowed to make it bigger
and so you have to allocate a new array
which we say takes linear time even if
allocating the new array didn't take
linear time
you have to copy all the elements over
from the old array to the new one then
you can throw away the old one
so just the copying from an array of
size n to an array of size n plus one
that will take linear time so static
arrays are really bad for
dynamic operations no surprise but you
could do them
okay that's static array now linked
lists are going to be almost the
opposite
well almost so if we store
the length okay we can compute the
length of the array very quickly
we can insert and delete at the front
really efficiently if i want to add a
new item
at the begin as a new first item
then what do i do i allocate a new node
which i'll call x
so this is insert
first of x
i'll allocate a new array size 2. i'm
going to
change let me do it in red
i'm going to change this head pointer uh
maybe i should do that
later so i'm going to set the next
pointer here to this one
and then i'm going to change this head
pointer to point to here
and boom now i've got a linked list
again we don't know anything about the
order and memory of these
these lists we just care about the order
that's represented implicitly by
following the next pointers repeatedly
so now i've got a new list that has x in
front and then x0 and then x1
and so on so insert and delete first at
least are really efficient
we won't get much more than that but a
linked list
insert and delete uh
first our constant time
so that's cool however everything else
is going to be slow
if i want to get the 10th item
in a linked list i have to follow these
pointers
10 times i go 0 0 1
2 3 and it's on follow ten next pointers
and i'll get the tenth item
accessing the item is gonna take order i
time
so uh
get and set at need
i time which in the worst case is theta
n
okay so we have sort of complementary
data structures here on the one hand a
static array
can do constant time get at set at so
it's very fast at the random access
aspect because it's an array
linked lists are very bad at random
access but they're
better at being dynamic we can insert
and delete at the beginning at least
in constant time now if we want to
actually insert a delete
at a particular position that's still
hard because we have to walk to that
position
even inserting and deleting at the end
of the list is hard
although that's fixable
and maybe i'll leave that for problem
session or problem set
but uh an easy so here's here's a small
puzzle
suppose you wanted to solve uh get
last efficiently in a linked list
how would you solve that in constant
time
yeah double a linked list is a good idea
but actually not the right answer
that's an answer to the next question i
might ask yeah
store pointer to the last element that's
all we need here
and often a doubly linked list has this
i usually call this the tail
head and tail and if you always just
store a pointer
to the last list this is what we call
data structure augmentation where we add
some extra information to the data
structure and
mean we have to keep it up to date all
the time so if we do
an insert last or something insert last
also becomes easy because i can just
add a new node here and update the
pointer here delete last is trickier
that's where you need a doubly linked
list
but whenever i add something to the end
of this list i have to update the tail
pointer also
as long as i maintain this now suddenly
get last as fast in constant time
so link lists are great if you're
working on the ends even dynamically
arrays are great if you're doing random
access and nothing dynamic nothing
adding
or deleting at the ends or in the middle
okay so our final goal for today
is to get sort of the best of both
worlds with dynamic arrays we're going
to try to get
all the good running times of linked
lists and all the good running times of
static arrays
we won't get quite all of them but most
of them
[Applause]
and in some sense
another way to describe what these
introductory lectures are about is
telling you about how python
is implemented so what we're going to
talk about next dynamic arrays
i've alluded to many times but these are
what python calls lists
so you don't have to implement a dynamic
array by hand because it's already built
into many fancy new languages
for free because they're so darn useful
this lecture is about
how these are actually implemented and
why they're efficient
and in recitation notes you'll see how
to actually implement them if all you
had were static arrays
but luckily we have dynamic arrays so we
don't have to
actually implement them but in the in
inside the python interpreter
this is exactly what's happening okay so
the idea is to
relax the
constraint or the invariant whatever
that the size of the array we use
equals n which is the number of
items in the sequence
okay remember in the sequence problem
we're supposed to represent n items
with a static array we allocated an
array of size exactly n
so let's relax that let's not make it
exactly n let's make it
roughly n how roughly you can think
about for a while
uh but the from an algorithm's
perspective usually that when we say
roughly
we mean throw away constant factors and
that turns out to be the right answer
here it's not always the right answer
but we're gonna uh enforce
uh that the size of the array is
theta m probably also greater than or
equal to n
0.5 n would not be very helpful so it's
going to be at least
n and it's going to be at most
some constant times n 2n
10n 1.1 times n any of these constants
will work i'm going to use 2n
here but there are lots of options
uh and now things almost work
for free there's going to be one
subtlety here
and i'm going to focus on we're still
going to maintain
that the item of the array is
represents x i okay so this data
structure
let me draw a picture so we've got an
array of some size
the first few items
are used to store
the sequence but then there's going to
be some blank ones at the end
okay maybe we'll keep track of this so
the data structure itself
is going to have an array and it's going
to have a length
something like this so we're also going
to keep track of the length so
we know that the first length items are
where their data is and the remainder
are just are meaningless
okay so now if i want to go and do an
insert last
what do i do i just go to
a of length and set it to x
and then i increment length
boom easy constant time yeah
how do you know you have enough room
indeed i don't this was an incorrect
algorithm
but it's usually correct as long as i
have extra space this is all i need to
do
for insert last but uh
i'm also going to store the size
of the array this is the actual this
whole thing is size
and this part is length
so length is always going to be less
than or equal to size
and so there's a problem if length
equals size
then i don't have any space okay
just add to the end
unless
n equals size i'm using n
and length for the same thing uh so the
length here
is same as n that's our actual number of
things we're trying to
represent and size this is great this is
the interface
size this is what we're trying to
represent and this
is the representation size this is the
size of my array
these are the number of items i'm trying
to store in that array okay this is the
interface versus data structure
here's the interface here's the data
structure
okay cool um
what do i do in the case when n equals
size
i'm gonna have to make my array bigger
okay this should
sound just like static arrays
with static arrays we made our array
bigger every time we insert it
and that was this linear cost of
allocation
we're going to do that sometimes with
static rays we had to do it every single
time because size
equaled n now we have some flexibility
we're only going to do it sometimes
it's like uh cookies are a sometimes
food apparently
according to modern cookie monster um i
don't understand
but uh if n equals size
we're going to uh
uh allocate
a new array
of size
any suggestions bigger i like it
greater than size how much bigger
twice five
five things size plus five come on jason
trolling me all right so there are a
couple of natural
choices here one is a constant factor
larger you could use 1.1 or 1.01
or 2 or 5 or 10 they'll all work
or you could use jason's trolling answer
of size plus a constant like 5.
why is this bad
you'll have to do it again you'll have
to resize frequently
when five steps later so
uh in the original static array we were
reallocating every single time
that's like n plus one if we do n plus
five that really doesn't change things
if we ignore constant
factors now we'll have to spend linear
time every five steps instead of linear
time every one step
that's still linear time per operation
just we're changing the constant factor
okay whereas two times size well now we
have to think a little bit harder
um let's just think about the case where
we're
inserting at the end of an array so
let's say we do
n uh insert
last
from an empty array
when do we resize well at the beginning
i guess i didn't say what we do for an
empty array let's say size equals one
so we can insert one item for free as
soon as we insert the second item
then we have to resize that seems bad
immediately we have to resize
then we insert the third item okay now
let's draw a picture so we start with
one item
we fill it up uh then we
grow to size two because that's twice
one then we fill it up
immediately we have to resize again but
now we get start to get some benefit
now we have size four and so we can
insert two items before
we have to resize and now we're size
eight
and we get to insert four items before
we refill
so uh this is going to resize and again
resizes are
expensive both because we have to pay to
allocate the new array
i drew it as just extending it but in
fact we're creating a whole new
array and then we have to copy all of
the items over
so there's the allocation cost and then
the copying costs so it's linear either
way
but we're going to resize at n
equals 1 2 4 8
16. you know this sequence all the
powers of 2
because we're doubling that is exactly
powers of 2.
so we pay a linear cost
so this resize cost the allocation
and the copying is going to be it's
linear each time
so it's 1 plus 2 plus 4 plus
8 plus 16. okay really i should write
this as
a sum from i equals 1
to roughly log n log base 2 of n is lg
of 2 to the i
okay if you want a terminus here it's
roughly n
it's actually the next the previous
power of 2 of n or something
okay but uh that won't matter that'll
just affect things by a constant factor
what is the sum of two to the i
this is a geometric series anyone know
the answer
two to the uh top limit
uh plus one minus one yeah so this is
the identity
sum of two to the i uh from i equals one
to
k is two to the k plus one
plus one minus one so the plus one's
upstairs
minus one's downstairs an easy way to
remember this is if you think in binary
as we all should we're computer
scientists uh two to the i
is means you set the ith bit to one so
here's
uh here's a bit string this is the ith
bit
this is 2 to the i the zeros down here
if i sum them all up
what that means is i'm putting ones here
okay uh and if you think about what this
means this is up to k
from zero sorry i should do zero to be
proper
if i write so that's the left hand side
the right hand side is 2 to the k plus 1
which is a 1 here
and the rest 0s so if you know your
binary arithmetic you
subtract if you add one to this you get
this so if you subtract one from this
you get this
okay this is why this identity holds um
or the higher level thing is to say oh
this is a geometric series
so i know i you should know this i'm
telling you now
geometric series are dominated by the
last term
the biggest term if you have any series
you can identify as geometric which
means it's growing at least
exponentially
then in terms of theta notation you can
just look at the last
term and put a theta around it and
you're done so this is
theta the last term like 2 to the log n
which is theta n
cool linear time linear time for
all of my operations i'm doing n
operations here
and i spend linear total time to do all
the resizing
that's good that's like constant each
kind of the kind of is an important
notion
which we call
amortization
so i want to say an operation
takes
t of n amortized time
if um
let's say any k of those operations
take at most k
times t of n time this is a little bit
sloppy but
be good enough so the idea is here if
this works for n or k
to do n operations from an empty array
here
takes linear time which means uh
i would call this constant amortized
amortized means a particular kind of
averaging
averaging over the sequence of
operations so while individual
operations will be expensive
one near the end when i have to resize
the array is going to take linear time
just for that one operation
but most of the operations are cheap
most of them are constant
so i can think of charging that high
cost to all of the other operations that
made it happen
and so this is averaging
over the operation sequence every insert
last
over there uh only takes constant time
on average
over the the sequence of operations that
we do
and so it's almost constant it's not
quite as good as constant worst case
but it's almost as good and it's as good
as you could hope to do
in this dynamic array allocation model
let me put this into a table uh and
you'll find these in the lecture notes
also
so we have on the top the main
operations of the sequence interface
which we will revisit in lecture seven
we'll see some other data structures for
this
get ad and set ad in the first column
insert and delete first insert and
delete last
insert and delete at an arbitrary
position so
we've seen three data structures now
arrays
were really good at get at set at they
took constant time
that's the blue one we're omitting the
thetas here
all the other operations took linear
time no matter where they were
linked lists were really good at insert
and delete first they took constant time
but everything else took linear time in
the worst case
these new dynamic arrays achieve
get at and set out in constant time
because they maintain this invariant
uh here that a of i equals x
i so we can still do gets and set at
quickly
and we also just showed that insert last
is constant amortized
delete last uh
you don't have to resize the array you
could just decrease
length and boom you've deleted the last
item
it's not so satisfying because if you
insert n items and then delete n items
you'll still have an array of size theta
n even though your current value of n is
zero
um you can get around that with a little
bit more trickery which are described in
the lecture notes but
it's beyond the we're only going to do
very simple amortized analysis in this
class
to prove that that algorithm is also
constant amortized which it is
uh you'll see in o46 or you can find it
in the clrs book
uh that's it for today