all right welcome back to double o six
and our dynamic programming quadruple of
lectures we are
over halfway through uh into lecture
three of four
uh today we're going to follow up on
this idea
of problem constraints and expansion
that was
mentioned especially towards the end of
last lecture we saw
an example of sub-problem expansion by a
factor of two
in the two-player game with coins where
we wanted to
have two versions of the game one where
i go first and one where you go first so
this was
expanding the number of sub problems by
factor of two today we'll see a bunch
more examples of this idea
including in one setting we've seen
already which is bellman ford which you
can think of
as a dynamic program maybe it's a good
time to mention that bellman
invented dynamic programming in the 50s
same bellman in the bellman ford
algorithm this was actually independent
discovery by both of them
and other people so
he invented dynamic programming and then
a few years later he applied it to solve
the single source shortest paths problem
so we saw them in the other order we saw
single source shortest paths first
because it's a little easier
and now we're seeing the general
framework that this fits into
and so we'll see how how that works uh
why did bellman call dynamic programming
dynamic programming mostly because it
sounded cool
and he was trying to impress uh
government agencies giving him grants
i mean how can you argue with something
as cool sounding as dynamic programming
but there is some logic to it
programming is a reference to an old
form of this word which means
optimization and generally we're trying
to optimize things
and instead of optimizing according to
some static kind of
approach or program we're doing it
dynamically this is
a reference to the local brute force
we're doing to optimize at each stage
so you can't tell at the top whether
what you're going to do in the middle
and so it's kind of each each
sub-problem is is
behaving differently and so in that
sense dynamic
it sounds cool all right then we'll go
to all pair shortest paths we'll see a
new algorithm for that
that's not asymptotically any better but
it's nice and simple and another way to
a cool way to see uh subproblem
expansion and then we'll look at a
couple of sort of practical
problems uh parenthesizing arithmetic
expressions
and a real world problem piano and
guitar fingering so assigning a
fingering to how to play a piece
and we're going to do that with our sort
bot framework so quick recollection of
what that is
we define subproblems and we saw how to
do that for sequences we try either
prefixes suffixes or substrings
we prefer prefixes and suffix because
there's fewer of them
if there's more than one sequence we
take the product of those spaces
um and then the idea we're going to
stress today is we can always add
sub-problems to make the next step
easier
in particular adding constraints to
sub-problems in some sense lets us
remember things about the subproblem
that's calling us
or about the past is one way to think
about it or in general to remember state
just by adding more sub problems we can
remember more stuff
not just what things we're working on
but some context
and that's what we'll see lots of
examples of today it'll hopefully make
sense by the end of the lecture
then we need to relate these sub
problems with a recurrence relation
which we usually just call a relation
and the key idea here
is to come up with a question that if
you knew the answer to that question
you could reduce
the subproblem you're trying to solve to
smaller sub-problem solutions
so this question is sort of the
fundamental
aspect of some fundamental aspect of a
solution typically
when you're dealing with suffixes you
want to ask some question about the
first item s of i
when you're dealing with prefixes you
want to ask a question about some the
near the last
item s of i minus one and for sub
strings who knows somewhere in the
middle
we'll see an example of that today once
you have
this is a reveal late something coming
later
uh once you have identified such a
question the
dynamic programming approach is don't be
smart about how to answer that question
just locally brute force try all
possible answers to the question
for each one recurse and
take the best solution according to
whatever metric you're trying to do
typically minimization or maximization
another way to think of this local bird
force which i like to think in my head
so maybe
some people like this some people don't
is to think about guessing the answer
to the question and so you know maybe
the the answer could be zero one or two
and uh my algorithm will say guess which
is the right answer
and i'll assume that my algorithm
correctly guesses the
answer and analyze that
so we can think of the program as going
straight through uh you know
guessing the answer and then recursing
and then combining the solutions however
but then at the end of course we can't
assume that the guess was correct
in fact we have to loop over all those
guesses so it's the same thing just
if you think of it this way there's less
looping in your program but when you
analyze it definitely the loop is really
there you have to pay for all the
possible answers
okay uh then we need to make sure this
relation is acyclic
get a subproblem dag and i like to
specify an explicit topological order to
make that clear
we have base cases for the relation we
have to solve the original problem in
terms of these sub problems and then we
analyze the running time
usually as the number of sub-problems
times the non-recursive work in each
in the relation so recursion is free
because we're multiplying by the number
of subproblems you can also sum over the
sub-problems sometimes that gives you
a tighter bound and then of course we
also have to add on
the running time we spend in the
original problem
all right so that was quick recap now
one
problem we saw in this framework uh two
lectures ago the first lecture was
single source shortest paths in a dag
which a lot of dynamic programs actually
can be reduced to shorter
single short shortest paths on a dag in
fact the reverse is true
single source shortest paths and a dag
can be thought of the dag relaxation
algorithm we saw
is essentially a dynamic program where
the sub problems are delta of sv
the relation is delta of sv is the min
so what are we thinking about here
we're guessing this is sort of a new
uh phrase i want to add we're guessing
uh oh actually i wrote it right here
the last edge uv on a shortest s to v
path
right so delta of sv the problem we're
trying to solve is uh
find shortest s to v path and it's
you know it's some path and we don't
know what it looks like
but uh some feature of the solution of
this path we're trying to find is well
what's the last edge
it comes from some vertex u unless the
path is of length 0
and you know s equals v that's a special
case dealt with in the base case
otherwise there's some vertex u before v
we don't know what it is so we're going
to locally brute force
or guess what the right answer is so
look at all incoming edges
from u to v and for each of them take
the recursive shortest path from s to u
plus the weight of the edge okay so this
is the guessing or local brute force
perspective
and the reason this works is because g
is acyclic
and so it has a topological order
otherwise
if graph if the graph has a cycle and
that's the case we want to go to next
uh this recursive call from delta of sv
to delta of su
will have loops in it and so you'll
never evaluate this recursion you'll
never finish you'll never memoize
you'll be sad it'll take infinite time
alright so don't use this algorithm
unless you have a dag for dag it's great
it's linear time
so that was a little review now here is
a single source shortest paths in
general graphs which we know as
bellman ford but rephrased into the sort
bot
framework okay so uh we defined this
problem in the bellman ford lecture
delta sub k
of sv remember this was the weight of
the shortest path from s to v
that is restricted to use at most k
edges
this made the problem feasible we ended
up taking the product of the graph into
many into
you know all these different subproblems
in fact
but from the perspective of dynamic
programming we think of this as a
subproblem constraint
what we really want is the shortest suv
path but that's hard
so we're going to break it up into
smaller pieces for each k
between zero and v we're going to say
well let's think about this problem
restricted to use at most k edges
and we and for simple paths you know if
there are no negative weight cycles we
only have to go up to v minus one
uh we prove that and we know then we're
happy
but that's that's sort of the last thing
we want to solve so if we look down at
the original problem we want
it's delta sub v minus 1 of sv
for all v so though if we could solve
these sub problems as we saw with bowman
ford
we can solve the original problem unless
there are negative weight cycles and we
use delta sub v
of sv to check whether they're negative
weight cycles i don't want to repeat
that but that's the
all in the original problem and then we
can take
this relation that we wrote for
dag shortest paths and just port it over
here
so remember this for a general graph
this has cycles so we can't use it
because we're just referring to
arbitrary delta of svs
so there's no reason to expect no cycles
there
in fact the call graph is exactly the
graph g so it has a cycle if only if g
does
but now over here i'm writing exactly
the same formula
for this min but i added a sub k here
and a sub
k minus 1 here the observation being if
i've already guessed what the last edge
uv is for this path
then if this whole thing has length at
most
k then this piece has length at most
k minus 1. so i only need to
do solve delta sub k minus 1 of sv
here and so that's what i wrote sorry i
have s u that's what i wrote here
there's one other thing which is as i
mentioned a little while ago
it could be that we use fewer than k
edges and so
let's consider the case where we don't
follow a last edge
and we just add to this min the
uh shortest path using at most k minus
one edges that's one option
for having at most k edges if we wrote
equality here
then we would remove that like the last
problem session
okay good
so the key observation here is that this
recurrence
does not have cycles just by adding this
index
if we solve these problems in order of
increasing k
all of the references from delta sub k
are in terms of delta sub k minus 1
and so this is now magically acyclic
this is why bellman ford worked
this but now i think in a very pleasing
way we're taking our graph that cyclic
and by spreading it out over various
copies of k and referencing always the
smaller one
we get an acyclic graph acyclic
relations
we have base cases like normal and then
we can analyze the running time
in the usual way which is uh summing
so uh what is this uh how much does this
cost we're going to take this the cost
of computing the relation
which is the number of incoming edges to
v
that's this theta and then
some overall sub problems so sum over k
and sum over v
i wrote it as a sum instead of our
product because this thing
is theta e
right the sum of incoming edges over all
vertices is exactly
the size of e and then we sum there's no
k in this formula so we just multiply by
v and we get v
e so our good friend belmont ford recast
kind of the opposite way before we were
using relaxations now we're just writing
them in explicitly
but essentially the same computation
just in a different order
okay cool so those are reviews of
old algorithms but in this new framework
show how powerful it is let's look at
another example which is our friend
right here all pairs shortest paths
okay a few lectures ago we saw johnson's
algorithm which solves this
very well so
one option we could use is just
this same set of sub problems but for
all u and v
right so for u and v and v
that's what we really care about um and
then we could say
k up to v something like that
so this would work but it would give the
same running time as
running bellman ford v times this is one
solution we know
but not the best solution we know for
how to solve shortest paths
so this would give v squared
e which is at most
v to the fourth for dense graphs it is
theta v to the fourth
so this is v times v times e um but
worst case is v to the fourth
uh what i'd like to show you now is a
different way to solve this problem
almost identical but gives v cubed
running time
which for dense graphs is really good so
we're going to reduce this 4 down to a
3.
this is an algorithm called floyd
warshall
it's definitely not an obvious algorithm
it's a very cool idea
and it's a nice example of a different
way remember this is sub problem
expansion we took
the problems we cared about multiplied
by this choice of k
here we're going to do the same thing
but define it differently
we're going to define those sub problems
differently so first i want to number
the vertices
starting at 1 for convenience so let's
see why
and then we're going to define some sub
problems which are
delta i'll call it u v comma k
maybe to avoid conflict i will
write a d here this is just a definition
local to this algorithm
so i want the weight of
shortest s to v path
okay so far just the same as the problem
we actually want to solve this is
delta of sorry uv
but i'm going to add a constraint which
is
using only vertices
in uv
and 1 2
up to k so this is the divine
inspiration to define sub problems this
way
it's a different constraint than the one
we saw here
which was using at most k
edges so in some sense what's slow about
this algorithm
is that we have to loop over all the
incoming vertices
this is expensive you know costs order
the degree which ends up with an e term
we're going to try to convert that e
term into a v
term by just knowing
which vertex to come from which sounds
impossible
but it turns out if you write the sub
problems this way so
i'm naming the vertices and say well
let me just find a path that uses uv
and the vertex labeled one there's only
like two options i could go straight
from u to v
or i could go from u to one to v and
then how about with one and two and one
and two and three
the same vertices okay by
label instead of by counting them
slight tweak but it turns out this uh
speeds up the algorithm so let me first
tell you how many sub problems there are
okay this is uh v cubed sub problems
two choices for u and v and i have uh
v choices for sorry i have v choices for
u v choices for v and v choices for k
roughly v cubed of these
uh but the key thing is that sounds like
a lot
but the relation becomes cheaper now
because i can just write delta u
sorry d of uvk
is the min of two things
d of u v k minus one
and d of u
k k minus one
plus d of k
v k minus one
okay it's a strange it's a strange
formula because we're using
indices like k for vertices also um
as well as u and v for vertices but
we're also using k
as a counter but we can do this because
our vertices are numbered
okay so the idea is the following we
have vertex u
we have a vertex v and we've already
found the shortest path
that uses vertices 1 through k minus 1
that's what this quantity is so it could
be
and now we're thinking we're trying to
add in this vertex k and think about
what are all the paths that could go
from u to v using one up to k
well it could be the shortest path from
u to v using one up to k doesn't use
vertex k
that's the first option just use
vertices one up to k minus one maybe we
don't need
to use k or it could be that the path
goes through k
well it starts at u and it goes to k
and then it goes from k to v okay all
using
for simple paths if we assume there are
no negative weight cycles and you have
to run down and forward to detect them
but assuming no negative weight cycles
the shortest path from u to v through k
must start by using vertices less than k
and then use vertices less than k again
and that's what i've written here it's
from u to k using k minus 1
up to k minus 1 labels and from k to v
using labels up to k minus one okay the
cool thing here
is the min only has two terms so this
takes constant time
non-recursive work so this is uh
k is not on the shortest path and this
is k is on the shortest path
cool so this is constant non-recursive
work
so if we jump ahead to the running time
with this algorithm we get
the number of sub-problems which is v
cubed times
the number the amount of work per
sub-problem which is constant
so that's just a v cubed algorithm
for dense graphs this is really good
this is when e is v squared then
this is the same thing as v times e so
as good as bellman ford
uh it's not as good as johnson for
sparse graphs
sparse graphs remember or in general
with with
johnson we got v squared log n
plus v e and this is always spending v
cubed
but it's cool because it's a very simple
algorithm let's uh
quickly write the topological order
this is
all we need is to guarantee that we
solve problems in increasing k
order because every reference here is to
a smaller k
for the third argument um and so for
example you can just write
a tripoli nested loop k equals zero one
up to v and um
u and v and v and v
so if you wanted to write this algorithm
bottom up you just write this triple for
loop
and then plug in this recurrence
relation here
and think of d as a table as a
set mapping instead of uh as a function
call
and boom in four lines you've got your
algorithm except you also need a base
case
base case here is
uv0 so i have to define what that means
but
when k equals zero the one through k set
is empty
so all i'm allowed to use are use my
vertices u and v
and so there are three cases for this 0
if u equals v
it's w of uv if there's an edge
from u to v and it's infinity otherwise
okay but easy base case constant time
for each
and then the original problems we want
to solve
are delta u v sides of v
because i number the vertices one
through size of v and if k equals size
of v that means i get to use all my
vertices so that is
regular shortest paths this is assuming
no negative weight cycles
okay we already know how to do negative
weight cycle detection so i'm not going
to
talk about that again but then this will
be
my shortest pathways because uh yeah
i implicitly assumed here that my path
was simple because
i imagine that i only use k once zero or
one time
and that's true if there are no negative
weight cycles
cool and we already did the time part of
sort bot
so v cubed algorithm very simple
basically five lines of code and you've
got all pair shortest paths
and if your graph is dense this is a
great running time if your grass
graph is not dense you should use
johnson like you will
and or like you have implemented in your
problem set
yeah so how was this compared to just
running dijkstra's outdoor a bunch of
times
ah what about using dijkstra so let's
compute so uh running dijkstra v times
is
the running time of um
johnson
so running dijkstra a bunch of times is
great if your graph has
only non-negative edge weights then you
should just run dijkstra
you get this running time and for sparse
graphs this is superior
if you have negative edge weights you
should run johnson which is bellman ford
once and then dijkstra
v times and we're comparing this to
the cubed which we just got
so this i mean how these compare depends
on how v and e relate
so on the one hand maybe uh v
is theta e that's what i would call a
very sparse graph it's quite common
then the running time we get here
is uh v squared
log v okay roughly v squared
on the other hand if we have a very
dense graph v is theta
e squared which for simple graphs the
most we could hope for
uh then this running time
is v cubed
okay so if you know the v is near e
squared then this is giving you
v cubed anyway from the ve term so why
not just use
this algorithm and often you know a
priori whether your graph is very sparse
or very dense or somewhere in between
if it's somewhere in between you should
still use johnson's algorithm because
you're going to get the benefit from
sparsity
and only have to pay this ve instead of
the v cubed
but if you know ahead of time you know
constant fraction of the edges are there
then just use uh or you have a small
enough graph that you don't care
just run a floyd washall because it's
simple and fast
good question any other questions
so this is an example of sub problem
expansion a very non-intuitive one where
we
use some prefix of the vertices but
notice it's prefixes again right i
numbered the vertices from 1 up to v
and i took a prefix of those vertices so
i just solved the problem using prefix
vertices 1 through k so it's actually a
familiar idea
if if all you'd seen are all our dynamic
programming examples of prefixes
suffixes
substrings actually it's a pretty
natural way to solve for his paths
maybe even more natural than this
anyway all right enough shortest paths
let's solve two more problems that are
more in our standard wheelhouse they
will involve sequences of inputs
not graphs first one is arithmetic
parenthesization
so first let me define this problem
okay we are given a
formula with say plus and times let me
give you
an actual example seven
plus four times three
plus five okay
now when you read this because you've
been well trained
you think okay i'm going to multiply 4
and 3 first because that has a higher
precedence and then i'll add
the results up but uh what i'm going to
let you do is parenthesize this
expression however you want
for example you can add parentheses here
and here okay you must make a balanced
parenthesis expression
a valid way to to pair up or not just
para but a valid way to evaluate this
expression any ordering you want
uh i could be inconsistent i could for
example do this sum
and then do this product and then do
this sum
okay but some kind of expression tree
uh over this and each one evaluates to
something so this is
11 and this is eight and so this
is 88 and my goal is to maximize
that computation and i claim that this
is the way to maximize that particular
example
okay so let me write it in general and
get my notation to match
notes so given a formula
a0 star 1
a1 star 2
a2 and so on up to star
n minus 1 a n minus 1
where each
a i is an integer as we like in this
class
and each star i is either
plus or times okay so i'm using star as
a generic operator it's
it has chose star because it is the
superposition of star on time
on top of a time symbol so it's
clear so you're given some formula any
mixture of plus
and times that you like involving n
integers
and your goal is to place parentheses
to maximize the result
okay so you can try all the combinations
here if i
for example take the product of 4 times
3 i get 12.
for if i do that first i get 12. then if
i add 5 and 7 i get
24 which is less than 88.
and i checked them all and this one is
the maximum
for that example okay so
interesting problem um sort of it's a
bit of a toy problem but it's motivated
by lots of
actual problems which i won't go into
here
so to apply this framework
you know we need to identify some
sub-problems this is a
sequence problem we're given a sequence
of symbols
and so natural things to try our
prefixes suffixes and substrings
i'm going to jump ahead and think about
the relation first
i want to identify some question about
the sub-problem
or its solution that would let me reduce
to smaller sub problems
this is a little trickier this is very
different we're not always doing
something on the left or on the right
or we can't assume there's something
happening on the left because maybe we
take a product in the middle first
if i take a product in the middle first
then i have some result here
but i still have three things i have the
thing to the left i have the thing in
the middle
and i have the thing on the right it
turns out to be very messy to think
about what the first operation is
because we can think of this as a tree
um
where we take a product here we take a
sum
of 7 and 4
and 3 and 5 over here and then take the
product at the root
but i don't know what the tree is right
i only know these numbers and these
operators but i don't know how to
organize this tree
and so the idea is if you think of this
tree what is the one thing that's the
easiest to identify
it's the root the root corresponds to
the
last operation i do in this computation
the last thing i did was take a product
and that's a lot easier because uh if i
guess
who's at the root which operator is at
the root that naturally decomposes into
the left subtree
and the right subtree and those will
always be
substrings we kind of know this this
node corresponds to everything left of
this operator
and this sub substring or this
subtree corresponds to everything to the
right of the operator
so this is our idea
is we're going to guess
which operation
star i is evaluated
last
or in other words at the root
so this is a question it has uh
n possible answers i guess actually n
minus one
from operator one to operator n minus
one
and so uh we'll just brute force all
those choices
okay so i wanted to start here because
uh to realize that
if i choose some star i in the middle
which might be the right thing like in
this example
star i is the the middle one middle
operator
i naturally decompose into everything to
the left of that operator and everything
to the right of that operator
this is a prefix this is a suffix
so you might think oh my sub problems
are all prefixes and all suffixes but
that would be wrong
because if you have a bunch of operators
and say you choose this one to be last
so i have a prefix here and a suffix
here and then there'll be some
within this suffix i'll choose some
operator to be the root of that one
and i have a prefix and a suffix of this
suffix but
in particular i will have to evaluate
this sub-problem which is a prefix of a
suffix in other words
a substring so never use a mixture of
prefixes and suffixes
if you need both you probably need all
sub strings
so our sub problems
are going to be substrings
okay i'm not going to write the sub
problems quite yet because there's
another idea
we need so what do i need to do with a
substring
so i'm going to guess the middle
operator and then evaluate the left
substring evaluate the right substring
what am i trying to do with those
substrings i guess i'm trying to solve
this problem which is
place parentheses in order to maximize
the result and then return what the
result
is okay and i can use parent pointers to
reconstruct what the
parentheses actually are
so is it enough once i guess what the
last operator is
is it enough to maximize the part to the
right and maximize the part to the left
will that always maximize my sum or
product according to what this operator
is
and you think about it for a while yeah
if i want to maximize a sum
i should maximize the two parts and if i
want to maximize a product i should
maximize the two parts that seems right
except i didn't say that my integers are
positive
so it's true if the integers are
positive but to make this problem more
interesting
we're going to allow the integers to be
negative
so for example uh 7
plus minus 4 times 3
plus minus 5. so i just added a couple
of minuses
to a couple of the numbers here
then it's no longer best to pair them
this way so if i pair them this way
like this or if i add parentheses this
way i get
3 here and i get minus 2 here
so i get product of that as negative 6
which is probably not the maximum in
fact i can do better i believe by
doing the left operator last
so this is a i claim the best
parenthesization if i remembered it
correctly
so this is minus two times minus four
is eight
uh plus seven is fifteen
okay so i got a positive number
definitely better than the negative
number i got
and i claim this is the best and the key
property here is
when we take a product of two negative
numbers we get a positive number so
sometimes
you actually want to make things small
because small
might mean very negative you take two
very big
negative numbers very small negative
numbers in other words
you take their product you get a very
big product positively
because the signs cancel okay so this
seems tricky
we want to work on substrings but we
don't know whether we're trying to
maximize or you might think well maybe
i'm trying to maximize the absolute
value
but that's not good maybe overall on
this entire expression i get
negative a million and that's not what i
wanted i wanted to maximize the sum
so i need to i still need to solve the
max
uh evaluation that i can get the max
parenthesization
but i also need to solve the min
parenthesization
if i can solve max and min i'll know the
entire kind of range that i could get
and i really only i'll care about men
especially when it lets me go negative
but let's just solve it in all cases the
min and the max
and then just brute force the rest
that's what i'm going to write down
so that was some motivation on why we
are going to define subproblems this way
so i'm going to define x of i comma j
comma
opt to be
so opt here is going to be either
min or max and this is my
sub problem expansion i really just care
about max
at the very end but i'm going to care
about min along the way
and i j is going to specify my substring
so this is going to be the opt value
opt stands for optimum here or
optimization
uh the upped value i can get for the
substring
a i start plus one
a i plus one and so on
to uh star j minus one
a j minus one okay
being careful to get my indices correct
here and
i want uh 0 less than or equal to i
less than j less than or equal to n
i claim and opt
like this okay so i'm going to get the
mean value and the max value separately
those are two different sub problems
it's my expansion this is the constraint
i'm adding
and i'm only focusing on this substring
from i inclusive to j exclusive
okay so i claim those are good sub
problems
let's write down a recurrence relation
okay
so i want to write x of i j
off on the left
and i want to optimize so this will be
min or max
on a set of choices what is my set of
choices
well uh like i said i want to guess what
is the last
operation evaluated i wrote star i here
but star i
is already defined so i'm going to use
star k so i'm going to
guess what which of my operations
between
i plus 1 and j minus 1 is the last one
and i evaluate
and that decomposes into everything left
of k
so that would be x of i
uh comma k comma something
and then we will do operator star k
on the part after k which is from k
to j something
okay and i'm choosing between
i think it's i less than k less than j
so k is some operator in between because
i started i plus 1
and i end at j minus 1 so those are the
possible choices for
k i tried them all that's my local brute
force
and then i take what i can get on the
left what i can get on the right
and multiply or add them according to
whether that operator is
plus or times now should i
maximize or minimize this one should i
maximize or minimize this one
i don't know so i'm just gonna do more
local brute force
um i'll say well let's just say opt
prime
for the left or maybe i'll call it opt l
for left and opt r for the right part
and i'll just add this to my for loop
let's just try uh opt l
and opt r just take all possible choices
among min and max
you know you could think hard and for
addition for example
if you're maximizing you should really
only need to maximize the two parts and
if you're minimizing you
you can prove you only need to minimize
the two parts
but let's for multiplication it's messy
it could be
really any of the options because
sometimes
when you minimize you get a negative
term sometimes when you
sometimes you don't and so you know it
depends what you're trying to do
you have to consider all the signs but
we don't need to think hard
we can just try all options there's only
four choices for opt l and opt r among
min and max you could do mid min min max
max min and max max
so try it's just a multiplication by
four in this for loop
the big cost is actually this one
because there are j
minus i choices for k
there's a constant number of choices for
opt l and opt r
and you need to prove that this is
correct i won't do it here
but the idea is if you're trying to
minimize or maximize your
sum or product it's enough to know
what ranges these could come in the
optimal choice will always be an extreme
in that range
okay and that's we consider all of them
here and so we get
this recurrence now it needs a base case
and we need to check that it's acyclic
but topological order is just increasing
j minus i this is the usual order for
um for substring problems because this
is increasing length of the substring
so start with very tiny substrings here
we'll start with length one substrings
we just have an ai there
so that's going to be our base case and
you grow up to the entire string
and it doesn't matter how we order
relative to opt as long as we are
increasing in j minus i
because i decay and k to j will always
be strictly smaller than i to j and so
this will be
acyclic the base case is
x of i i plus 1 opt
this is always a i doesn't matter what
opt is because there's nothing there's
no choice
you just have a single number in that
substring because we're exclusive on i
plus 1.
okay and then the original problem we
want to solve
is x of 0 n
max could also solve min and see how
small you can get it
so if you wanted to maximize the
absolute value you could solve the max
problem and the min problem and take the
largest
of those two options and how much time
does this take
well how many sub problems are there for
substring problems we have n squared sub
problems now we multiplied the number of
subproblems by 2
but that's still n squared okay so we
have n
squared sub problems
and how much work per subproblem are we
doing well
uh as i mentioned we're we're doing j
minus i choices for k
constant number of choices for opt l and
opt r so this is theta j minus i
which if i'll be sloppy that's at most
big o of n
and it turns out to be the right answer
anyway so
uh there's linear amount of
non-recursive work
in fact it's like a triangular number
but uh that's still theta
uh n cubed
okay same running time as uh v cubed we
just got but
polynomial time and this is pretty
impressive because we're really brute
forcing all possible parenthesizations
they're about four to the n
exponentially many uh parenthesizations
of an expression
but we're finding the biggest the one
that evaluates to the largest value and
the one that
evaluates the smallest value in just n
cubed time polynomial
and a key here was subproblem expansion
where we in addition to solving the max
problem we also solved the min
problem because sometimes you want to
take
two very small negative numbers and
product them together to get a larger
positive number
cool question
would anything go wrong if i added minus
a divide
so what if i had operators minus and
divide
uh it's a good question i'm i'm certain
that
minus should work fine if we do min and
max this will still evaluate the
should still evaluate the largest thing
for division i need to think about the
cases
i would guess it works but what we need
to prove
is that the way to maximize or minimize
a division say given two numbers in the
left and right uh is
that it either corresponds to maximizing
or minimizing the thing on the left
and then maximize or minimizing the
thing on the right so as long as you
have this kind of
it's not exactly monotonicity it's just
that in order to compute max or min
it suffices to know the maximum
of the two parts it's like interval
arithmetic you know interval arithmetic
i want to know what is the what are the
extremes i can get on
the output of a division if i'm given
that a number is in some interval here
and some interval here
if the answer is always use one of the
extreme endpoints here and use one of
the extreme endpoints here then this
algorithm will work otherwise
all bets are off cool
so if you negate if you put a minus here
that will work fine because it's just
negating this range and then it's
just like sum but
oh divided be careful about zero yeah
actually so it doesn't work because
we care about how close this can get to
zero for division
it might be enough to to consider those
it's like instead of minimizing
and instead of computing this entire
interval if this interval spans 0
maybe i need to know if zero is here
i need to know how close to zero i can
get on the left side and how close to
zero i can get on the right side still
just four quantities i need to know
i would guess for division that's enough
yeah nice
solid little problem so then we'd be
multiplying the sub problem space
instead of by two
by four hey maybe we should put this on
the final no
now it's in lecture so we can't use it
but
it's a cool it's a cool set of problems
right you can do a lot with dynamic
programming
just uh you don't need to be that clever
just brute force
anything that seems hard and when it
works it works great
all right and this class is all about
understanding when it works
and when it doesn't work of course we
will only give you problems where it
works
but it's important to understand when it
doesn't work for example
dag shortest paths that algorithm on a
non-dag
very bad infinite time okay
our last example is piano fingering
so here we're given
a sequence of notes
t zero t one t for note
up to n t n minus one um
these are single notes
and all the single notes all the single
notes
right and we have fingers on our
hands this is not like two-finger
algorithm this is the
five-finger algorithm uh
so in general i'm going to assume an
arbitrary
uh anthropomorphic
object so this is five for humans
most humans some humans i think the
maximum on each hand
is seven could be smaller maybe you've
had an accident
okay i'll solve it for arbitrary f
and what we'd like to do is assign
fingers to notes
to tell our pianist which finger to use
for each note
so normally when you're given sheet
music it just gives you
a sequence of notes you want to play it
doesn't tell you which finger you want
to play it with
unless you have some nice uh training
booklets and they have a little number
on top of each and then you number them
one two three four five and
symmetrically one two three four
five and so here's a giant piano
for my giant hands um so uh
if jason were here he could sing these
notes so maybe i play this
with my first finger this with my second
finger let's just say i'm doing a scale
so i can walk and now i think typical
way to do a scale is to reach over with
your
first finger second finger i guess and
then do something like this
no okay clearly i don't know scales
um or how to play a piano but you know
there's there's limits here i can
if i'm going from this note and i want
to go to another note
uh over here okay maybe i have a decent
span
from first finger to fifth finger but my
span from my first finger to my second
finger is
not as big i can't reach as far so if i
want to play this note and then this
note i'd like to
start here with a very extreme finger on
the left
and then go to a very extreme finger on
the right so
i'm going to formalize this problem
pretty abstractly because i don't want
to get into
music performance i'm going to say that
there's a metric
d for if i'm at note t
with finger f and i want to go to note t
prime with finger f
prime then this function
d of t f t prime f prime gives me the
difficulty of doing that
the difficulty of playing
note t with finger f
and then playing
note t prime with finger f prime
it's w is width uh so this is a
transition difficulty
i'm not going to worry about difficulty
of the whole piece other than saying
i've got to play this note then i've got
to play this note and for now just
single notes
you play a single note with your right
hand then you play another single note
with your right hand then another single
note with your right hand
let's assume no pauses for now no rests
and great so we uh
have this difficulty from going from the
ith note to the i
plus first note and then our goal is to
minimize the sum of difficulties
minimum sum of
d t i f i
t i plus 1 f i plus 1.
and these f i's and f i plus 1
is what we want to compute we don't know
which fingers to use we're only given
which notes to play okay so this is
actually a natural problem there are
lots of papers about this problem i've
read a bunch of them
obviously not super well about how to
compute how to play scales
but um there are notes like are there
there are constraints in this usually
people write this
metric as a sum of different penalty
terms
if i want to minimize difficulty
difficulty is high
if i play a note far on the left
on the left so if i go from a low note
to a high note
that's easier to do if i use a lower
numbered finger and go to a higher
numbered finger you don't want the
you don't want to go like i was doing
from
a high numbered finger to a low numbered
finger
to play a note on the right that's
annoying so i'd like to do an assignment
if i can that avoids that so i'll just
have some penalty of like 100
if that happens and zero if it doesn't
happen sum up a bunch of terms like that
other examples are avoid the fourth and
fifth fingers the weak fingers
or if i'm playing a portion of the song
that is
legato then i don't want to use the same
finger to play
two notes uh right after the other i've
gotta use two different fingers for that
so you have a penalty if f i equals f i
plus one
and these two notes are not the same uh
then
and we're in legato mode then we add a
penalty term
and things like that i would prefer if
i'm going from a very low note to a very
high note
i'd like to use more extreme fingers
things like that
okay but we're just going to assume this
di d function is given to us
it's some you know const was
some polynomial size if you imagine the
notes on your keyboard
are n notes or m notes then some
polynomial and m
size to this function okay
so how do we solve this problem
i'm running low on time so let me
give you
the idea
and this is going to use sub-problem
expansion
so the sub-problems are going to be
x of i comma f this is the minimum
total difficulty
to play a suffix because i like suffixes
ti up to tn minus one
uh starting
with finger f
uh on note ti
okay so the obvious sub problems would
be without this
constraint this here is a sub problem
constraint
and you could try to define the sub
problems just as what's the best way to
play a suffix
but i claim it's important to know which
finger we start with so we're going to
multiply the number of sub-problems by
capital f which is just five so
very small sub-problem expansion and
then we're going to constrain each
sub-problem to say well what if i
started with my first finger what if i
started with my second finger what if
up to the fifth finger try them all
okay then i turns out i can write a
relation
which is x of i f
equals the min what should i what should
i min
over i'll just uh guess
this i'm already told what my first
finger is
to use which finger i should use for ti
so what's the next thing that matters
well i guess what finger to use
for the ti plus one the very next note
which what is the next finger i use
i will call that f prime and minimize
over f prime between one and capital f
of the remaining suffix
starting with f prime plus my difficulty
function
from t i comma f to t i plus 1
comma f prime end of bracket
okay so there's kind of a lot going here
this is a
lowercase f prime
but actually if you think about what i'd
like to write the recurrence on
i start with the suffix i i like to
recurse on the smaller suffix so that's
x of i plus one
and so here uh if if i know that i'm
parametrizing by finger number
for i well then i in order to even call
this function i need to know what finger
i'm using for i plus one so once you
decide on these sub problems it's really
obvious you need to guess
what is the next finger and then recurse
on that finger recurs on the remaining
suffix of that finger
now why did we need to know what f what
these
fingers were why not just guess what the
first finger is
uh well it has to do with this
difficulty function for this difficulty
function i know that i want to measure
the difficulty from t i to t i plus one
and to do that because this function is
parametrized by four things i need to
know both the finger for ti
and at the same time the finger for ti
plus one
if i remove this f i could you know add
a min over one finger but i can't really
add a
min over two fingers so what this does
by parameterizing by f here and writing
down the optimal
for each starting finger f i can
at some sense i'm remembering
in this call what finger i started with
because i told it you have to start with
finger f prime and so locally to
x i f um
i know what finger f prime is being used
for t i plus pos1 and also
because of the definition of x i f i
know what finger i'm using for ti
and so i get to know both of these
fingers one comes out of this min
and the other is given to me as this
parameter
and then of course if i can solve these
problems i can solve the original
problem
by just one more min
of 1 up 1 up to capital f
of x 0 little
f right i don't know which finger to
start with but that's just
f choices and so then this recurrence
gives me
the overall solution in i'll just jump
to the time you need a base case
and uh topological order but it is
n f squared time there are n
times f sub problems here and for each
one
i'm doing an optimization over f choices
so i get n times f
squared it's a polynomial and if f is a
constant this is actually linear
time very fast dp now what i described
here is for
one hand one note at a time but you can
generalize this to
two hands each with one note well that's
just ten fingers so
uh you could solve this separately for
the right hand and left hand if you know
which notes are being played
with left-handed right hand but some
pieces that's not obvious
so to make it more interesting what if
you have multiple notes
at the same time and let's say i think
it's reasonable so you can only play
up to f notes at a time
one for each finger and so we have an
upper bound of the number of notes
at a time that we're playing which is
good
oh i have a drawing of this dp by the
way
as a subproblem dag so
this is the original problem which is we
don't know which finger to
start with but then we just have a
complete bipartite graph here
where we write on each of these edges
what is the difficulty of this
transition
the y axis here is which finger i'm
using one two three four five
and the x-axis is which suffix i'm
considering which note
do i start on and so you could solve
this with shortest paths and a dag
or you could just solve it directly this
dp either top down or bottom up
okay jumping ahead
so if you do multiple notes at a time
instead of this finger choice which just
had f choices
we have
what do i write here t
to the f uh
possible states
where t is the maximum number of notes
that i could play at once
and usually this is at most f one for
finger but we could generalize
and this is you know number of fingers
so i could deal with all 10 fingers of a
typical
human set of arms hands
and say there's at most 10 and so this
is 10 to the 10. it's a
big constant but it's constant uh like a
billion
billion right 10 billion
but then is that times n and maybe that
squared times
n will let me exhaustively enumerate all
the possible things i could do
be doing with all of my hands so you can
apply this not only to
piano fingering but also to guitar
fingering also to rock band rock band is
an easy case where you just have five
buttons and usually only four fingers
that you use
and this doesn't really make any sound
so it's not that exciting
so the case where f equals four
is uh you can apply to optimally figure
out which
which finger once you have a difficulty
function of what transitions are easy
and hard
for rock band then you can optimally
figure out your fingering for rock band
songs
with a real guitar this is a little bit
harder and
because there are actually multiple ways
to play the same note
for example i can play
the note of this string like this
these should both sound the same my
guitar were
perfectly tuned which is not
and that's you know properties of
strings and the way these things are
are played so in addition to what finger
i use i should also decide which string
to play that note on if all i'm given is
sheet music different notes to play
another thing i could guess is which
string to play that note on
so for example maybe i want to play my
favorite song
here
the super mario brothers my favorite
song um
i can keep going but i actually can't
keep going it won't be as impressive
uh i don't actually know how to play
guitar uh but the
there are a lot of choices there right
um i started with uh playing this note
down on this um down on this string
that's good i could have also played it
on this string
but that's more work for my finger so i
have a diff penalty function that says
well if i play an open string that's a
lot easier
and then i had a transition from here
this note
to this note to this note and if you
focus on my fingering here i chose to
use my
index finger for the first one because
that index finger is always the easiest
to use
but also gives me lots of room to move
my pinky over to here
and then i like to use my middle finger
to come up here
you could also use your index finger
it's just a little bit more of a reach
so you have to define some difficulty
function it might depend on how
big your fingers are you can do that and
then optimize using these algorithms
what is the best guitar fingering for a
given piece
one note at a time or several notes at a
time you could even add in parameters
like
oh maybe i want to play a bar
that's not a perfect bar but this would
be great for my
playing this note down here and this
note up here
bad example uh so you could do other
things with the guitar to make it more
interesting
and you know generalize this dynamic
program suitably
but i think this gives you a flavor how
with subproblem expansion i can capture
almost
any aspect of a problem that i want as
long as the number of states that i
need to keep track of is small i can
just multiply the number of subproblems
by that state
and i can keep track of any kind of
transition from one state to another
which i could also do with taking the
product of a graph
but dynamic programming gives you a kind
of methodical way to think about this
by figuring out some property in this
case the state of how my
how my fingers are applied to the
instrument
and then just sort of brute forcing the
rest a very powerful
framework
you