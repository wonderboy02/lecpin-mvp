all right welcome back to double06 data
structures
today we're going to cover a different
kind of
tree-like data structure called the heap
binary heap it's going to let us solve
sorting problem in a new way let me
first
remind you of a portion
the problem we're going to be solving
today is called priority queue
this is the interface we'll see several
data structures but one main data
structure for today
and this is a subset of
the set interface
and subsets are interesting because
potentially we can solve them better
faster simpler something
and so you'll recognize
you should recognize all of these
operations except we don't we didn't
normally highlight
the max operation so here we're
interested in storing a bunch of items
they have keys which we think of as
priorities
and we want to be able to identify the
maximum priority
item in our set and remove it
and so there's lots of motivations for
this maybe you have a router packets
going into the router they have
different priorities assigned to them
you want to route the highest priority
first
or you have processes on your computer
running trying to run on
your single threaded single core
and you've got to choose which one to
run next and you usually run higher
priority processes first
or you're trying to simulate uh a system
where events happen at different times
and you want to process the next event
ordered by time all of these are
examples of the priority queue interface
we'll even see applications within this
class when we get to graph algorithms
but the general the main two things we
want to be able to support are inserting
an item which
includes a key and deleting the maximum
item and also returning it at the same
time
we'll also talk some about being able to
build the structure
faster than just inserting it but of
course we could implement build by
starting
empty and repeatedly inserting and
also the complexity of just finding the
max without deleting it
this you could simulate with these two
operations by deleting the max and then
re-inserting it
which works but often we can do faster
okay but the two key main operations are
insert
and delete max and we're going to see a
few data structures to do this
any suggestions among the data
structures we've seen in this class what
should we
use to solve priority queue interface
any possible answers
sequence avl oh that's interesting uh
sequence fl is a good answer
but that's maybe the fancier version
yeah set avl sounds good
set avl supports these operations and
many more
all in log n time except for build which
takes
n log n time because you have to sort
first uh
so set avl is a good good way to do this
we'll talk we'll come back to your
sequence avl
idea later uh this gets login
for operation great i mean this is
set avl is our most powerful data
structure it does all the operations we
care about
uh on the set side and the sequence of
vl does all the operations on the
sequence side but note that this is a
set not a sequence
we care about keys there are hacks to
get around that with sequence avls but
let's do that later
so great if we wanted to for example
speed up find max
in a set avl we could add augmentation
we could
remember subtree property augmentations
we can use that to get constant time
find max
by storing in every node the maximum
key item within the subtree and that's a
subtree property it's one we mentioned
last
class so we could even improve that to
constant time
great um so we're done end of lecture
in some sense that's true but what we're
going to see today
is another data structure called the
binary heap which is in some sense a
simplification of
set avl it achieves basically the same
time bounds
build will be faster by a log factor
but that's not the main reason we care
about them
the main advantage is that they're
simpler and they give us
an in-place sorting algorithm
so i have up here the three of the
operations i've been talking about build
insert and delete max
uh so we have set avl trees there and
log n build
log n insert log n delete so along the
way to
our heap i want to mention two other
data structures one is
a dynamic but unsorted array and the
other is a dynamic sorted array
these are simpler data structures we've
talked about many times before
and they're useful kind of motivations
for getting started because
a heap is going to be built on top of
arrays instead of what's sort of a
fusion between arrays and trees
so um if i have an unsorted array
this is very easy to insert into right i
just
append to the end this is what we called
insert last
so insert is fast
constant amortized we might have to
resize the array but
so that's the amortized part but delete
max is slow in unsorted array i don't
know where the maximum is so i have to
scan through the whole array
so i scan through the array identify the
max is somewhere in the middle
and then if i want to delete it
i want to delete that maximum element
well in a dynamic array all i can really
do is delete the last element
efficiently so i could for example swap
it with the
last element so i take this element and
put it here
and then delete the last element in that
array
which is pop in python or delete last in
our
world so overall this is
linear time which is bad
but i wanted to highlight exactly how
it's done for a reason we'll get to in a
moment
sorted array is sort of the reverse it's
very easy to find the max
where is it at the end
delete max uh the maximum element is
always the last
element in a increasing sorted array
i guess that's constant amortized
because then i have to delete it which
may incur
resizing insert
though is going to be linear because
maybe i can binary search to find where
that item where the added item belongs
let's say i just added
this item here um i could bind our
search to find it but then i'm going to
have to do a big shift so i might as
well just
swap repeatedly until i find the
position where
the added item x belongs
and now i've restored sorted order that
takes linear time
which is bad and what we want is somehow
the best of these two worlds
insert is fast for array uh
delete is fast for a sorted array we
can't get constant time for both but we
can get login time for both we already
know how
with set avl trees but we're going to
see a different way to do it today
and the main motivation for a different
way to do this
is sorting so i want to define
a priority queue sort
so given any data structure that
implements a priority queue interface in
particular insert
and delete max i can make a sorting
algorithm
what do i do insert all the items delete
all the items
but because when i delete them they come
out largest first
i get them in reverse sorted order then
i could reverse in linear time
and i've sorted my items so we can
insert
x for x and a
or build a
and then repeatedly
delete max
okay how much time does this algorithm
take i'm going to
introduce some notation here it takes
however long it takes to build and items
call that t sub build uh
n plus uh
sorry
plus n
times the time to do a delete max
okay or uh we can write this as
n times time to do an insert
plus time to do a delete max
okay so i'm using these t functions to
just abstract
what are the running times provided by
my data structure that implements this
interface
the interface says what's correct is and
these t functions to give me my
performance bounce
so if i plug in each of these data
structures
i get a sorting algorithm i get avl sort
i get
array sort i get a sorted array sort
what do those
look like it turns out many of these are
familiar
uh so set avls take
login for operation so we get an n login
sorting algorithm out of them which is
insert all of the items into the abl
tree i don't want to use avl build
because that uses
sort i'm not allowed to sort in order to
implement sort
but we saw how to insert into an abl
tree and keep the thing balanced
so that takes log n each and then we can
find the max delete it rebalance
and so on total time will be n log n
this is an algorithm we call avl sort
it's a bit complicated because avl trees
are complicated
but it gives us optimal comparison bound
and login
now what about
array sort so suppose i use an
unsorted array i
insert the item so if i insert the items
so i'm doing all the insertions here
before all the deletions so what's going
to happen is i just insert the items in
the original array order in other words
i just take the array
and then what i do is repeatedly
extract the maximum item
by searching for it moving it to the end
of the array
and then repeating that process that's
unfamiliar
that's selection sort from lecture
three um so this
arrays give us selection sort
this is a new way to think about what we
were doing way back then
with a sorted array
what are we doing we insert all the
items that's actually where
all the work happens because we maintain
the sorted array so we start with an
empty array it's sorted
we add an item okay still sorted we add
a second item and we swap if we need to
in order to sort in general we add an
item we swap it to the left until it's
sorted again
that is insertion sort
okay kind of cool this is a unifying
framework for
three uh sorting algorithms that we saw
before we didn't
actually talk about avl sort last time
but it was in the notes
and so that is the right part of this
table so of course
uh these array data structures are not
efficient they take linear time some for
some of the operations so the sorting
algorithms are not efficient
but they're ones we've seen before so
it's neat to see how they fit in here
uh they had the selection sort of
insertion sort had the advantage that
they were in place you just needed a
constant number of pointers or indices
uh beyond the array itself so they're
very space efficient
so that was a plus for them but they
take n squared time so you should never
use them
except for like n at most 100 or
something
avl tree sort is great and then it gets
n log
end time probably more complicated than
merge sort and you could stick to merge
sort
but neither merge short nor set avl tree
sort
are in place and so the goal of today
is to get the best of all those worlds
in sorting to get n log n comparisons
which is optimal in the comparison model
but get it to be in place
and that's what we're going to get with
binary heaps
we're going to design a data structure
that happens to build a little bit
faster as i mentioned linear time
building
so it's not representing a sorted order
in the same way that avl trees are
but it will be kind of true based it
will also be array-based
we're going to get logarithmic time for
insert and delete max it happens to be
amortized because we use arrays
but when the key thing is that it's an
in-place data structure
it only consists of an array of the
items
and so when we plug it into our sorting
algorithm
priority q sort our generic sorting
algorithm not only do we get n log n
performance but we also get it in place
sorting algorithm this will be our first
and m and only this class
and log n in place sorting algorithm
cool that's the goal
let's do it so uh
what we're going to do uh because we're
in place basically we have to have an
array
storing our n items that's sort of the
definition of in place
just using n slots of memory exactly the
size of
the number of items in our structure but
we're obviously not going to use a
regular unsorted array or a regular
sorted array we're going to use array
just as sort of the underlying
technology
for how things are stored but we'd
really like
logarithmic performance which should
make you think tree
only way to get a log is with the binary
tree more or less
so tree
somehow we want to embed a tree
into an array let me grab an example
let me draw a tree
if i got to choose any old tree i want i
would choose
a tree that's basically perfectly
balanced
perfectly balanced to be like this where
what's the property that i have all of
these
levels all of these depths are
completely filled with nodes
this is depth zero
remember this is depth one this is depth
two
this is depth three so what i'd really
like
uh is to have uh two to the i nodes
at depth i that would be a perfect
binary tree uh but that only works when
n is one less than a power of two
right uh i can't always achieve that for
any n
and so the next best thing i could hope
for is
2 to the i knows a depth i until the
very last i
the largest depth and in that
level i'm still going to restrict things
i'm going to force
all of the nodes to be as far left as
possible
okay so i want to say accept
at max depth
where
nodes are i'll call them left justified
and these two properties together is
what i call a complete
binary tree
uh why is this interesting because i
claim i can represent a tree like this
as an array i've narrowed things down
enough
that i can draw an array down here
and what i'm going to do is write these
nodes in depth order
so write a first because that's step
zero then bc that's depth one
then uh well they're alphabetical i made
it that way
d e f g is depth two
and then hij is depth three
okay this is very different from
traversal order of a tree
traversal order would have been h d i b
j e
a f c g okay but this
is what we might call depth order do the
lowest depth nodes first
very different way to lay things out or
to to linearize
our data and this is what
a heap is going to look like so the cool
thing is
between complete
binary trees and arrays is a bijection
for every array there's a unique
complete binary tree
and for every complete binary tree
there's a unique array
why because the complete constraint
forces everything
forces my hand there's only if i give
you a number n
there's one tree shape of size n right
you just fill in the nodes
top down until you get to the last level
and then you have to fill them in left
to right
what you might call reading order for
writing down nodes
and the array is telling you which keys
go where
this is the first node you write down at
the root this is the next node you write
down at the left child of the root
and so on okay so here we have a binary
tree
represented as an array or an array
representing a binary tree
the very specific binary tree it has a
clear
advantage which is it is guaranteed
balance no rotation is necessary in
heaps because
complete binary trees are always
balanced in fact they have the best
height they possibly could
which is ceiling of log n
balanced remember just meant your big o
of log n this is
one times log n so it's the best level
of balance you could hope for
so somehow i claim we can maintain a
complete binary
tree for solving priority queues this
would not be possible if we were trying
to solve the whole set interface
and that's kind of the cool thing about
heaps is that by just focusing on the
subset of the set interface
we can do more we can maintain this very
strong property
and because we have this very strong
property we don't even need to store
this tree we're not going to store left
and right pointers
and parent pointers we're just going to
store the array
this is what we call an implicit data
structure
which basically means no pointers
just an array
of the n items
how are we going to get away without
storing pointers i'd still like to treat
it like a tree i'd still like to know
the left child of b is d and the right
child of b
is e we'll see why in a moment
we can do this with uh
index arithmetic so
maybe i should add some labels before i
get there
so this array naturally has indices
right this is index zero
this is index one index two index three
index four
index five index six seven eight
nine because there are ten items zero
through nine
and i can apply those labels up here too
these are the same nodes so 0
1 2. this is just depth order
but once i have this labeling it's going
to be a lot easier to figure things out
so if i wanted to know the left child of
b is d somehow
given the number one i want to compute
the number three
uh add two they're all sort of
multiplied by three there are all sorts
of operations that take one and turn it
into three
but there's only one that's going to
work in all cases and the intuition here
is
well i have 2 to the i nodes at level i
if i want to go to the child
level there's 2 to the i plus 1 nodes
down there right exactly double
except the very last one but that won't
really matter if there is a left child
it will behave the same
and so intuitively i have this space of
size 2 to the i have to expand it to a
spacer size
to the i plus 1 so i should multiply by
2.
okay and that's almost right but then
there's some
constants so i'd like to say 2 times i
but if we look at the examples here 1
times 2 is
2 which is 1 less than 3. 2 times 2 is 4
which is 1 less than 5. hey
we almost got it right it's just off by
one
off my one is you know index errors are
the most common things in computer
science
what about the right child
if the left child's a two i plus one
where's the right child
i hear lots of mumbles two i plus two
one more because we're writing things
left to right in depth order
the right child is the right sibling of
the left child so it's just one
one larger okay given those rules
we can also compute parent it's just
whatever
is the inverse of both of these
functions
which i want to divide by two at some
point
but be to get i want to get back to i
given 2i plus 1 or given 2i plus 2.
and so um
if i subtract 1 from i
then i either get 2i or 2i plus 1 and
then if i take an integer division by 2
i get i the original i sorry maybe i'll
call this j
to be clearer so j is the left or right
child
then i can reconstruct i which was the
parent
okay so this is you know constant number
arithmetic operations so i don't have to
store
left and right pointers i can just
compute them whenever i need them
whenever i'm at some node like e
and i want to know what's its left child
if sorry i
given the node index 4 which happens to
contain
the item e and i want to know what's
this left child i just multiply by 2 and
add 1 i get 9.
and then i can index into this array at
position nine because i don't this
is just in my head remember uh this is
we're just thinking that there's a tree
here
but in reality on the computer there's
just the array
so if we wanna go from e to j we can
from four to nine
uh if we go try to go to the right child
we multiply by 2
8 add 2 10 and we see oh 10 is beyond
the end of the array
but our race stores its size so we
realize oh e does not have a right child
this is something you can only do in a
complete binary tree in a general binary
tree you don't have these nice
properties
cool so this is basically
a heap i just need to add one more
property
naturally called the heap property
so uh there are
multiple types of heaps this type of
heap is called a binary heap
we will talk about others in future
lectures i'm going to call it
q
i should write
explicit thing this is an array
representing a complete binary tree
call the array q and we want
every node
to satisfy
the so-called max heap property
which says q of i
is greater than or equal to q of j
for both children left of eye
and right of eye
okay so um we have a node
i and has two children
two i plus one and two i plus two
these are our two values of j
what we want is a greater than or equal
to up
uh relation here and here
okay so this node should be bigger than
both this one and this one which of
these is larger
we don't know and we don't care very
different from
binary search trees or set binary trees
where we said
these guys were less than equal to this
this one this one was less than equal to
all the nodes in the sum tree here we're
just locally saying
this node is greater than or equal to
this node and this node so this is the
biggest is at the top
okay so one nice
lemma about these heaps this is
weird let me give you some more
intuition
if you are a binary heap if you satisfy
this max heap property everywhere
then in fact you learn that every node i
is greater than or equal to all nodes in
its subtree these are what we call
descendants
and subtree of i
okay let me look at this example
so i haven't written any numbers here
but you can imagine
so a here is greater than or equal to
both b
and c and b is greater than equal to d
and e
and c is greater than or equal to f and
g and d is greater than equal to h and i
and e is greater than equal to j
that would make this structure a heap
not just a complete binary tree
so what does that imply it implies that
a must be the maximum
so you look at any node here like j a is
greater than or equal to b
is greater than or equal to e is greater
than or equal to j
and in general what we're saying is that
a is greater equal to
all nodes in the tree b is greater than
or equal to all nodes in its sub tree
down here
c is greater than or equal to all nodes
in its subtree that's what this lemma is
saying
you can prove this lemma by induction uh
but it's really simple
if you have two nodes i and j and j is
somewhere in the sub tree
that means there's some downward path
from
i to j and you know that for every edge
we traverse on a downward path
our key is going down non-strictly
because every child is less than equal
to its parent i is greater than or equal
to this is greater than equal to this is
greater equal this is greater than or
equal to j
okay so by uh transitivity of less than
or equal to
you know that i is in fact greater than
equal to j or sorry the key
in i is greater than or equal to the key
in j
this is what we're calling i the index
this is what we call q
of i this is index j
q of j
okay very different way to organize um
keys in a tree
but as you might imagine this is going
to be good for priority queues because
priority queues just need to find the
maximum element
okay then they need to delete it that's
going to be harder because deleting the
root is like that's the hardest node to
delete intuitively
i'd really prefer to delete leaves but
deleting leaves
and keeping a complete binary tree is
actually kind of hard right
if i want to delete h that doesn't look
like a binary tree
or it doesn't look like a complete
binary tree anymore it's not left
justified
similarly if i want to delete f that's
bad because now i don't have four
nodes here the one node that's easy to
delete is j
right if i remove that node i still have
a complete tree the last leaf
the last position in my array is the one
that's easy to delete
that's good because arrays are good at
deleting the last item
but what i've set up here is it's easy
to find the max it's going to be up here
at the root
deleting it is annoying i'd like to
somehow take that key
and put it at position at the last
position at the last leaf
because that's the one that's easy to
delete
okay and that's indeed what we're going
to do do in a delete
algorithm uh let me first do insert i
guess that's a little simpler
kind of symmetric
[Applause]
to what we just said so if i want to
insert a key
or an item x which has some key
uh again the only thing i really can do
in an array if i want to add a new item
it has to go at the end
the only thing we know how to do is
insert at the end of an array this is
what we called insert last
this corresponds to adding no a node
containing
x the item x at the
in the very last level of the binary
complete binary tree
either it goes to the right of all the
existing nodes or it starts a new level
but it's always going to be the last
leaf after we do the insertion it will
be at position
size of q minus one
okay this is probably not enough though
we just insert an arbitrary item in a
leaf and now it may not satisfy the max
heap property anymore
so let's just check if it does and if it
doesn't fix it
that's what we know how to do but this
time
we're not even going to need
we're not even going to need rotations
which is cool
so i'm going to define an operation
called max heapify up
this will make things more like a max
heap
we're going to start at size of q minus
for our value i but it's going to be
recursive
so what we're going to do is look at a
node i
in particular the one that just got
inserted and where could it violate
things well with its parent
because we have no idea what key we just
put here maybe it's less than our parent
then we're happy
but if it's greater than our parent
we're in trouble
and we should fix it so
if
the item in the parent
this key is less than
eyes key
ah i see i forgot to write key in all
these spots this should be dot key
and dot key because q of i is an item
let's guess it's key so this is the bad
case this is if the parent is smaller
than the child
we wanted the parents to always be
greater than or equal to its children
so in that case uh what can we do
swap them
let's swap q parent of i
excellent more chuck uh with q of i
now they're in the right order okay now
we need to think about what about the
other child
of that node and what about its parent
okay so um i have some numbers here
let's say this was 5
and this was 10. what do i know about
this picture before
well i know that 10
is this newly inserted item it's the
only one that could have caused
violations
when i first inserted it so i know that
before this before i
moved 10 around i knew all the things in
this left subtree
are less than or equal to 5 and
everything up here
are greater than or equal to 5.
i also know that the nodes in here in
fact were less than or equal to five
other than
this node ten that we just inserted uh
this was a correct heap so five was a
separator between
things above it on the ancestor chain
are are greater than or equal to five
and things in its
sub tree are less than or equal to it
okay so after i do this swap
which i'm just going to do
after i swap the items 5 and 10.
10 is up here 5 is here and now i
realize okay great this edge is happy
because now 10 is greater than or equal
to 5. but also this edge is happy
because it used to be happy and we only
made its parent larger
okay now this edge maybe is bad
and so we need to recurse
recurse on the parent
but that's it okay so we fixed this one
edge initially this happens way down at
the leaf but in general
we're taking our item that we inserted
which is x
and it starts at the last leaf and it
maybe bubbles up for a while
maybe it gets all the way to the root if
we inserted a new maximum item
but in each step it goes up one and so
the running time of all this stuff
is the height of the tree which is log n
okay and because there's only this one
item that could potentially be wrong if
it ever stops moving we've just checked
that it satisfies the maxi property
if it gets to the root you can also
check it satisfies the maxi property so
there's a base case i didn't write here
which is
if i equals zero we're at the root uh
we're done
okay and then you can prove this correct
by induction
there's just one item that's in the
wrong spot initially and we put it into
a right spot there are many places it
could go
but we will move it to the i guess
unique ancestor position that is correct
that satisfies maxi property okay so
that's insert
delete is going to be
almost the same
delete man that is
sorry delete max thank you
you can of course define all of these
things for min instead of max everything
works the same
i just have a hard time remembering
which one we're doing
um just don't switch you can't use a max
heap to do delete min you can't use the
mini heap to do delete max but
you can use a min heap to do deletemin
that's fine
so like i said the only le
the only node we really know how to
delete is the last leaf and the last
level which is the end of the array
because that's what arrays can delete
efficiently
and what we what we need to delete is
the root item
because that's always the maximum one
which is at the first position in the
array
so what do we do swap them our usual
trick
i think the cool thing about heaps is we
never have to do rotations we're only
going to do swaps which is something we
had to do with trees also
binary trees
sorry q of 0 with
q of the last item
great done now we have the last item is
the one we want to delete so we do
delete last
or pop in python
and boom we've got we've now deleted the
maximum item
of course we may have also messed up the
maxi property
just like we did with insert
so with insert we were adding a last
leaf
now what we're doing is swapping the
last leaf with the
i'm pointing at the wrong picture let me
go back to this tree
what we did is swap item j
with a so the problem is now and then we
deleted this node
the problem is now that that root node
has
maybe a very small key because the key
that's here now
is whatever was down here which is very
low in the tree so intuitively that's a
small value
and this is supposed to be the maximum
value we just put a small value in the
root
so what do we do heapify down
uh we're going to take that item and
somehow push it down
to the tree until the down in the tree
until maxi property is satisfied
so this is going to be max
keepify down and we will start
at position zero which is the root
uh and max heapify down
is going to be a recursive algorithm so
we'll start at some position i
but initially that's the root and what
we're going to do
is look at position i and its two
children
so let's say we put a very small value
up here like zero
and let's say we have our children five
and ten we don't know
maybe i'll swap their order just to be
more generic
because that looks like well not quite a
minor search tree but
we don't know their relative order but
one of them is greater than or equal to
the other
because they're in some order and so
what would i like to do to fix this
local picture
yeah i want to swap and i could swap
zero is clearly in the wrong spot it
needs to go lower in the tree i can swap
zero with five or zero with ten
which one ten
i could draw the picture with 5 but
it will not be happy
y10 we want to do it with the larger one
because then this edge will be happy
and also this edge will be happy if i
swapped five up there instead the 510
edge would be unhappy
it wouldn't satisfy the maxi property so
i can do one swap and fix
maxi property except that again zero
may be unhappy with its children zero
was this one item that was in the wrong
spot
and so it may have to go farther down
but five will be
five didn't even move so it's happy
everything in this subtree is good
uh what about the parent well if you
think about it
because everything was correct a correct
heap before we added 0 or before we put
0 too high
all of these nodes will be greater than
or equal to 10
and so on the ancestor path
and all of these nodes were less than
equal to 10 before
and less or equal to 5 so that's still
true
but you see this tree is happy this tree
still may be unhappy zeros still might
need to push down farther
that's going to be the recursion okay so
we
check
[Applause]
here
okay there's a base case which is if i
as a leaf we're done
because there's nothing below them uh
so we satisfy the maxi property at i
because there's no children
otherwise let's look at the leaf
among the left sorry left not leaf
among the two children left and right of
eye right of i might not exist then
ignore it
but among the two children that exist
find the one that has maximum
key value q
of dot key
okay that's that was 10 in our example
and then if
these items are out of order if we do
not satisfy
so greater than would be satisfy less
than q of j
would be the opposite of the maxi
property
here if if maxi property is violated
then we fix it by swapping
q of i with q of j
and then we recurse on j
okay call max heapify down of j
that's it so pretty symmetric uh insert
was a little bit simpler because we only
have one parent
delete min because we're pushing down we
have two children we have to
pick one but there's a clear choice the
bigger one
and again this algorithm this whole
thing will take order h time the height
of the tree which is log n
because our node just sort of bubbles
down at some point it stops when it
stops we know
the maxi property was satisfied there
and if you check along the way
by induction all the other maxi
properties will be satisfied
because they were before okay
so almost forced what we could do here
the amazing thing is that you can
actually maintain a complete
binary tree that satisfies the maxi
property
but once you're told that the algorithm
kind of falls out because
we have an array the only thing we can
do is insert and delete the last item
and so we've got to swap things to there
in order to or out of there in order to
make that work
and then the rest is just checking
locally that you can fix
fix the property
cool uh so
that's almost it not quite what we
wanted
so we now have login amortize insert and
delete max
in our heap we did not yet cover linear
builds
right now it's n log n if you insert n
times
and we did not yet cover how to make
this an in-place sorting algorithm
so let me sketch each of those
i think first is in place
so
how do we make this algorithm in place i
guess i want that
but i don't need this
so we want to follow priority queue sort
do you want that
but i don't want to have to uh
grow and shrink my array i would just
like to start with the array itself
so this is in place
so what we're going to do is say okay
here's my array
that i want to sort that's given to me
that's the input
to priority queue sort
and what i'd like is to build a priority
queue out of it initially
it's empty and then i want to insert the
items one at a time
let's say okay so uh
in general what i'm going to do is
maintain that q is some prefix of a
that's going to be my priority queue
it's going to live in this
subarray this prefix okay so how do i
insert a new item
well i just increment
so to do an insert the first step
is increment size of q
then i will have taken the next item
from a and injected it into this queue
and conveniently
if we look at our insert code which is
here
the first thing we wanted to do was add
an item at the end of the array so we
just did it
without any actual work just conceptual
work we just said oh our q is one bigger
boom now this is at the end of the array
no more amortization in fact because
we're not
ever resizing our array we're just
saying oh now q is a little bit bigger
of a prefix
it just absorbed the next item of a
similarly
delete max
is going to at the end decrement the
size of q
why is that okay because at the end
of our delete max operation not quite at
the end but almost the end
we deleted the last item from our array
so we just replaced that delete last
with
a decrement and that's going to shrink
the queue by
one it has exact same impact as deleting
the last item but now it's constant time
worst case not amortized
and the result is we never actually
build a dynamic array we just
use a portion of a to do it so what's
going to happen is
we're going to absorb all the items into
the priority queue and then start
kicking them out
as we kick them out we kick out the
largest
key item first and we put it here then
the next largest then the next largest
and so on the minimum item's going to be
here
and boom it's sorted this is the whole
reason i did max heaps instead of min
heaps
is that in the end this will be a upward
sorted array with the max at the end
because we always kick out items at the
end
we delete the max first okay so that is
what's normally called heap sort
you can apply this same trick to
insertion sort and selection sort and
you actually get the insertion sort and
selection
sort algorithms that we've seen which
operate in prefixes
of the array okay cool
so now we have we've achieved the y up
there which is n log n
sorting algorithm that is in place so
that was our main goal
keep sort let me very quickly mention
you can build a heap in linear time
with a clever trick so if you insert the
items one at a time that would
correspond to
inserting down the array and every time
i insert an item i have to walk
up the tree so this would be the sum
of the depth of each node
if you do that you get n log n
okay this is the sum over i of log i
that turns out to be n log n it's
log of n factorial the cool trick
is to instead imagine adding all the
items at once and not heapifying
anything
and then heapify up or sorry heapify
down
from the bottom up so here we're
heapifying
up now we're going to heapify down
and surprisingly that's better because
this is the sum of the heights of the
nodes
and that turns out to be linear it's not
obvious but
intuitively for depth this is zero this
is log n
and we've got a whole ton of leaves so
right at the leaf level you can see
we're paying n log n
right because there are n of them and
each one costs log n
down here at the leaf level we're paying
constant because
um the height of the leaves are one here
the height of the root is log n and this
is
this is better now we're paying a small
amount for the thing
of which there are many and it's not
quite a geometric series but it turns
out
this is linear so that's how you can do
linear building heap
to come back to your question about
sequence
avl trees turns out you can get all of
the same bounds as heaps
except for the in-place part by taking a
sequence avl tree storing
the items in an arbitrary order and
augmenting by max
which is a crazy idea but it also gives
you linear build time
and yeah there's other fun stuff in your
notes but i'll stop there